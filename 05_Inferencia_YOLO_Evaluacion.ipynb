{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Inferencia con YOLO y Evaluación de Métricas\n","En este notebook, realizaremos inferencia usando un modelo YOLO preentrenado para detectar objetos en imágenes. También calcularemos métricas como IoU, precisión, recall y mAP para evaluar la calidad de las detecciones."]},{"cell_type":"markdown","metadata":{},"source":["## Instalación de la Librería YOLO\n","Utilizaremos la librería `ultralytics`, que simplifica el uso de YOLOv5/v8. Esta librería permite realizar entrenamiento, inferencia y evaluación de manera eficiente."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install ultralytics"]},{"cell_type":"markdown","metadata":{},"source":["## Importar Librerías y Configurar el Modelo\n","Cargaremos un modelo YOLO preentrenado y lo utilizaremos para realizar inferencias."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from ultralytics import YOLO\n","import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","import requests\n","\n","# Cargar un modelo YOLO preentrenado\n","model = YOLO('yolov8n.pt')  # Modelo pequeño preentrenado en COCO"]},{"cell_type":"markdown","metadata":{},"source":["## Cargar y Procesar Imágenes\n","Seleccionaremos imágenes de ejemplo para realizar la detección de objetos."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["url1 = 'https://images.pexels.com/photos/210182/pexels-photo-210182.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1'\n","url2 = 'https://images.pexels.com/photos/30192151/pexels-photo-30192151/free-photo-of-traditional-dance-in-sarajevo-celebration.jpeg'\n","\n","def download_image(url):\n","    response = requests.get(url, stream=True).raw\n","    image = cv2.imdecode(np.asarray(bytearray(response.read()), dtype=np.uint8), cv2.IMREAD_COLOR)\n","    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","image1 = download_image(url1)\n","image2 = download_image(url2)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","axes[0].imshow(image1)\n","axes[0].axis('off')\n","axes[0].set_title('Imagen 1')\n","axes[1].imshow(image2)\n","axes[1].axis('off')\n","axes[1].set_title('Imagen 2')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Inferencia con YOLO\n","Realizaremos la detección de objetos en las imágenes cargadas y visualizaremos los resultados con cajas delimitadoras y etiquetas."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Realizar detección en tus imágenes\n","results1 = model(image1)\n","results2 = model(image2)\n","\n","# Obtener las cajas predichas (predictions)\n","pred_boxes1 = [box.xyxy[0].tolist() for box in results1[0].boxes] if results1[0].boxes is not None else []\n","pred_boxes2 = [box.xyxy[0].tolist() for box in results2[0].boxes] if results2[0].boxes is not None else []\n","\n","# Visualizar resultados para la primera imagen\n","annotated_image1 = results1[0].plot()\n","plt.imshow(annotated_image1)\n","plt.axis('off')\n","plt.title('Detecciones - Imagen 1')\n","plt.show()\n","\n","# Visualizar resultados para la segunda imagen\n","annotated_image2 = results2[0].plot()\n","plt.imshow(annotated_image2)\n","plt.axis('off')\n","plt.title('Detecciones - Imagen 2')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluación de Métricas\n","Calcularemos métricas clave para evaluar la calidad de las detecciones realizadas por YOLO:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_iou(boxA, boxB):\n","    xA = max(boxA[0], boxB[0])\n","    yA = max(boxA[1], boxB[1])\n","    xB = min(boxA[2], boxB[2])\n","    yB = min(boxA[3], boxB[3])\n","    interArea = max(0, xB - xA) * max(0, yB - yA)\n","    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n","    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n","    return interArea / float(boxAArea + boxBArea - interArea) if float(boxAArea + boxBArea - interArea) > 0 else 0\n","\n","def calculate_precision_recall(pred_boxes, true_boxes, iou_threshold=0.5):\n","    tp = 0\n","    fp = 0\n","    fn = 0\n","    \n","    for pred_box in pred_boxes:\n","        matched = False\n","        for true_box in true_boxes:\n","            iou = calculate_iou(pred_box, true_box)\n","            if iou >= iou_threshold:\n","                tp += 1\n","                matched = True\n","                break\n","        if not matched:\n","            fp += 1\n","    fn = len(true_boxes) - tp\n","    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","    return precision, recall\n","\n","def calculate_average_precision(precisions, recalls):\n","    precisions = np.array([0] + precisions + [0])\n","    recalls = np.array([0] + recalls + [1])\n","    for i in range(len(precisions) - 1, 0, -1):\n","        precisions[i - 1] = max(precisions[i - 1], precisions[i])\n","    indices = np.where(recalls[1:] != recalls[:-1])[0]\n","    return np.sum((recalls[indices + 1] - recalls[indices]) * precisions[indices + 1])"]},{"cell_type":"markdown","metadata":{},"source":["### Cargando caja real de la imagen2\n","\n","\n","La anotación está creada con una herramienta que genera los recuadros con formato COCO, entonces hay que transformas este recuadro para que el modelo lo cargue correctamente."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def transform_bbox(bbox):\n","    \"\"\"\n","    Transforma una bounding box del formato [x_min, y_min, width, height]\n","    al formato [x_min, y_min, x_max, y_max].\n","    \"\"\"\n","    x_min, y_min, width, height = bbox\n","    x_max = x_min + width\n","    y_max = y_min + height\n","    return [x_min, y_min, x_max, y_max]\n","\n","bboxes_coco = [\n","    [881.9361084220718, 469.2352371732813, 1353.9980638915815, 2450.7647628267187],\n","    [2298.121974830587, 350.5130687318486, 1218.3155856728022, 2569.4869312681512],\n","    [16.96030977734753, 678.4123910939009, 1048.7124878993227, 2241.587608906099],\n","    [3550.3581800580805, 734.9467570183938, 829.6418199419195, 2185.053242981606]\n","]\n","\n","# Convertir todas las bounding boxes\n","true_boxes2 = [transform_bbox(bbox) for bbox in bboxes_coco]\n","print(\"Bounding Boxes transformadas:\", true_boxes2)"]},{"cell_type":"markdown","metadata":{},"source":["Visualizamos la imagen con las cajas reales"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def draw_boxes(image, boxes, title=\"Imagen con Bounding Boxes\"):\n","    # Crear una copia de la imagen para no modificar la original\n","    img_with_boxes = image.copy()\n","    \n","    # Dibujar cada caja delimitadora\n","    for box in boxes:\n","        xmin, ymin, xmax, ymax = box\n","        cv2.rectangle(img_with_boxes, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (255, 0, 0), 20)  # Color azul\n","        \n","    # Mostrar la imagen con las cajas\n","    plt.imshow(img_with_boxes)\n","    plt.axis(\"off\")\n","    plt.title(title)\n","    plt.show()\n","\n","\n","# Visualizar las cajas para Imagen 2\n","draw_boxes(image2, true_boxes2, title=\"Ground Truth - Imagen 2\")"]},{"cell_type":"markdown","metadata":{},"source":["<strong>NOTA</strong>: En este notebook, nos enfocamos en evaluar únicamente las predicciones correspondientes a la clase \"person\" (ID 0 en COCO). Esto evita penalizar al modelo por predicciones correctas de otras clases y asegura que las métricas reflejen el rendimiento para la clase objetivo.\n","\n","A menudo, los modelos como YOLO detectan múltiples clases de objetos en una imagen. Sin embargo, si nuestro interés está en evaluar el rendimiento del modelo para una sola clase específica (por ejemplo, \"person\"), debemos filtrar las predicciones para excluir otras clases.\n","La función filter_predictions_by_class permite hacer esto fácilmente. Se proporciona el ID de la clase objetivo (en este caso, 0 para \"person\") y devuelve únicamente las cajas correspondientes a esa clase."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def filter_predictions_by_class(results, class_id):\n","    \"\"\"\n","    Filtra las predicciones por clase específica.\n","    Args:\n","    - results: Predicciones del modelo YOLO (formato ultralytics).\n","    - class_id: ID de la clase que se desea filtrar (ej. 0 para \"person\").\n","    Returns:\n","    - Lista de cajas filtradas en formato [xmin, ymin, xmax, ymax].\n","    \"\"\"\n","    filtered_boxes = []\n","    for box in results.boxes:\n","        if int(box.cls) == class_id:  # Compara con el ID de clase deseado\n","            filtered_boxes.append(box.xyxy[0].tolist())\n","    return filtered_boxes\n","\n","# Clase \"person\" tiene ID 0 en el conjunto COCO\n","CLASS_ID_PERSON = 0\n","\n","# Filtrar predicciones de la clase \"person\" para ambas imágenes\n","filtered_pred_boxes2 = filter_predictions_by_class(results2[0], CLASS_ID_PERSON)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Ejemplo de Evaluación\n","Vamos a calcular precisión, recall y mAP para la segunda imagen usando cajas reales."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cálculo de métricas\n","precision2, recall2 = calculate_precision_recall(filtered_pred_boxes2, true_boxes2)\n","average_precision = calculate_average_precision([precision2], [recall2])\n","print(f'Precision: {precision2:.2f}, Recall: {recall2:.2f}, mAP: {average_precision:.2f}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def draw_boxes_side_by_side(image, true_boxes, pred_boxes, true_color=(0, 255, 0), pred_color=(255, 0, 0), thickness=3):\n","    \"\"\"\n","    Dibuja las bounding boxes reales y predichas en imágenes separadas y las muestra juntas.\n","    Args:\n","    - image: Imagen original.\n","    - true_boxes: Lista de cajas reales (ground truth) en formato [xmin, ymin, xmax, ymax].\n","    - pred_boxes: Lista de cajas predichas en formato [xmin, ymin, xmax, ymax].\n","    - true_color: Color de las cajas reales (por defecto verde).\n","    - pred_color: Color de las cajas predichas (por defecto rojo).\n","    - thickness: Grosor de los recuadros.\n","    \"\"\"\n","    # Crear copias de la imagen original\n","    img_with_true_boxes = image.copy()\n","    img_with_pred_boxes = image.copy()\n","    \n","    # Dibujar las cajas reales (ground truth)\n","    for box in true_boxes:\n","        xmin, ymin, xmax, ymax = map(int, box)\n","        cv2.rectangle(img_with_true_boxes, (xmin, ymin), (xmax, ymax), true_color, thickness)\n","    \n","    # Dibujar las cajas predichas\n","    for box in pred_boxes:\n","        xmin, ymin, xmax, ymax = map(int, box)\n","        cv2.rectangle(img_with_pred_boxes, (xmin, ymin), (xmax, ymax), pred_color, thickness)\n","    \n","    # Mostrar las imágenes lado a lado\n","    _ , axes = plt.subplots(1, 2, figsize=(15, 7))\n","    axes[0].imshow(img_with_true_boxes)\n","    axes[0].axis('off')\n","    axes[0].set_title(\"Ground Truth - Imagen 2\")\n","    \n","    axes[1].imshow(img_with_pred_boxes)\n","    axes[1].axis('off')\n","    axes[1].set_title(\"Predicciones - Imagen 2\")\n","    \n","    plt.show()\n","\n","# Mostrar la imagen 2 con recuadros reales y predichos lado a lado\n","draw_boxes_side_by_side(\n","    image2,\n","    true_boxes2,          # Cajas reales\n","    filtered_pred_boxes2,      # Cajas predichas filtradas\n","    true_color=(0, 255, 0),  # Verde para ground truth\n","    pred_color=(255, 0, 0)   # Rojo para predicciones\n",")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}
